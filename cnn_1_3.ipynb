{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#id-2 - [1,0], id_7 - [0,1]\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_x, data_y = [], []\n",
    "img_length = 500*500*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "folder = 'pixiv/id_2'\n",
    "for img_file in os.listdir(folder):\n",
    "    img=np.array(Image.open(folder+'/'+img_file))\n",
    "#     img=np.array(Image.open('/'.join([folder, img_file])))\n",
    "#     img = np.reshape(img, 500*500*3)\n",
    "#     img = np.reshape(img, len(img)*len(img[0])*len(img[0][0]))\n",
    "#     print(len(img))\n",
    "#     print(img)\n",
    "    data_x.append(img/255.0)\n",
    "    data_y.append([1,0])\n",
    "print(len(data_x))\n",
    "print(len(data_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "folder = 'pixiv/id_7'\n",
    "for img_file in os.listdir(folder):\n",
    "    img=np.array(Image.open(folder+'/'+img_file))\n",
    "#     img=np.array(Image.open('/'.join([folder, img_file])))\n",
    "#     img = np.reshape(img, 500*500*3)\n",
    "#     img = np.reshape(img, len(img)*len(img[0])*len(img[0][0]))\n",
    "#     print(len(img))\n",
    "#     print(img)\n",
    "    data_x.append(img/255.0)\n",
    "    data_y.append([0,1])\n",
    "print(len(data_x))\n",
    "print(len(data_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.36470589  0.36078432  0.58823532]\n",
      "   [ 0.32549021  0.32156864  0.54901963]\n",
      "   [ 0.3137255   0.30980393  0.53725493]\n",
      "   ..., \n",
      "   [ 0.31764707  0.32156864  0.50196081]\n",
      "   [ 0.32941177  0.33333334  0.51372552]\n",
      "   [ 0.36862746  0.37254903  0.5529412 ]]\n",
      "\n",
      "  [[ 0.29803923  0.29411766  0.52941179]\n",
      "   [ 0.27450982  0.27058825  0.49803922]\n",
      "   [ 0.27843139  0.27450982  0.50196081]\n",
      "   ..., \n",
      "   [ 0.25490198  0.25882354  0.43921569]\n",
      "   [ 0.25882354  0.26274511  0.44313726]\n",
      "   [ 0.29411766  0.29803923  0.47843137]]\n",
      "\n",
      "  [[ 0.3137255   0.30980393  0.54509807]\n",
      "   [ 0.27450982  0.27058825  0.50588238]\n",
      "   [ 0.26274511  0.25882354  0.48627451]\n",
      "   ..., \n",
      "   [ 0.25490198  0.25882354  0.44705883]\n",
      "   [ 0.26666668  0.27058825  0.45882353]\n",
      "   [ 0.30588236  0.30980393  0.49019608]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.24705882  0.23921569  0.38431373]\n",
      "   [ 0.20784314  0.2         0.34509805]\n",
      "   [ 0.2         0.19215687  0.34509805]\n",
      "   ..., \n",
      "   [ 0.22352941  0.21176471  0.39607844]\n",
      "   [ 0.23137255  0.21960784  0.40392157]\n",
      "   [ 0.27450982  0.26274511  0.43921569]]\n",
      "\n",
      "  [[ 0.24705882  0.23921569  0.38431373]\n",
      "   [ 0.20392157  0.19607843  0.34117648]\n",
      "   [ 0.19607843  0.1882353   0.34117648]\n",
      "   ..., \n",
      "   [ 0.21960784  0.20784314  0.39215687]\n",
      "   [ 0.21960784  0.20784314  0.39215687]\n",
      "   [ 0.25882354  0.24705882  0.42352942]]\n",
      "\n",
      "  [[ 0.3019608   0.29411766  0.43921569]\n",
      "   [ 0.25882354  0.25098041  0.39607844]\n",
      "   [ 0.25098041  0.24313726  0.39607844]\n",
      "   ..., \n",
      "   [ 0.28627452  0.27450982  0.45882353]\n",
      "   [ 0.28235295  0.27058825  0.45490196]\n",
      "   [ 0.31764707  0.30588236  0.48235294]]]\n",
      "\n",
      "\n",
      " [[[ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   ..., \n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]]\n",
      "\n",
      "  [[ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   ..., \n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]]\n",
      "\n",
      "  [[ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   ..., \n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   ..., \n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]]\n",
      "\n",
      "  [[ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   ..., \n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]]\n",
      "\n",
      "  [[ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   ..., \n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]]]\n",
      "\n",
      "\n",
      " [[[ 0.98431373  0.99215686  0.98823529]\n",
      "   [ 0.99607843  1.          1.        ]\n",
      "   [ 0.98431373  0.98823529  0.99607843]\n",
      "   ..., \n",
      "   [ 0.88235295  0.88627452  0.90588236]\n",
      "   [ 0.89803922  0.90196079  0.92156863]\n",
      "   [ 0.87843138  0.88235295  0.90196079]]\n",
      "\n",
      "  [[ 0.98431373  0.99215686  0.98823529]\n",
      "   [ 0.99607843  1.          1.        ]\n",
      "   [ 0.99215686  0.99607843  1.        ]\n",
      "   ..., \n",
      "   [ 0.91764706  0.92156863  0.94117647]\n",
      "   [ 0.93333334  0.93725491  0.95686275]\n",
      "   [ 0.90588236  0.90980393  0.92941177]]\n",
      "\n",
      "  [[ 0.98431373  0.98823529  0.99607843]\n",
      "   [ 0.99607843  1.          1.        ]\n",
      "   [ 0.98823529  0.99215686  1.        ]\n",
      "   ..., \n",
      "   [ 0.90980393  0.92156863  0.94117647]\n",
      "   [ 0.9254902   0.93725491  0.95686275]\n",
      "   [ 0.90588236  0.91764706  0.93725491]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.8392157   0.89411765  0.93725491]\n",
      "   [ 0.86274511  0.91764706  0.96078432]\n",
      "   [ 0.82352942  0.87843138  0.92156863]\n",
      "   ..., \n",
      "   [ 0.68627453  0.73333335  0.73333335]\n",
      "   [ 0.6901961   0.73725492  0.73725492]\n",
      "   [ 0.69411767  0.74117649  0.74117649]]\n",
      "\n",
      "  [[ 0.81176472  0.86666667  0.90980393]\n",
      "   [ 0.83529413  0.89019608  0.93333334]\n",
      "   [ 0.80784315  0.86274511  0.90588236]\n",
      "   ..., \n",
      "   [ 0.69411767  0.74117649  0.74117649]\n",
      "   [ 0.7019608   0.74901962  0.74901962]\n",
      "   [ 0.70588237  0.75294119  0.75294119]]\n",
      "\n",
      "  [[ 0.82352942  0.87843138  0.92156863]\n",
      "   [ 0.84313726  0.89803922  0.94117647]\n",
      "   [ 0.80784315  0.86274511  0.90588236]\n",
      "   ..., \n",
      "   [ 0.70588237  0.75294119  0.75294119]\n",
      "   [ 0.71764708  0.7647059   0.7647059 ]\n",
      "   [ 0.72156864  0.76862746  0.76862746]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[ 0.98431373  0.98431373  0.97647059]\n",
      "   [ 1.          1.          0.99215686]\n",
      "   [ 0.97647059  0.97647059  0.96862745]\n",
      "   ..., \n",
      "   [ 0.9137255   0.90196079  0.87450981]\n",
      "   [ 0.93333334  0.92156863  0.89411765]\n",
      "   [ 0.92156863  0.90980393  0.88235295]]\n",
      "\n",
      "  [[ 0.98039216  0.98039216  0.97254902]\n",
      "   [ 1.          1.          0.99215686]\n",
      "   [ 0.98823529  0.98823529  0.98039216]\n",
      "   ..., \n",
      "   [ 0.93333334  0.92156863  0.89411765]\n",
      "   [ 0.94901961  0.93725491  0.90980393]\n",
      "   [ 0.92156863  0.90980393  0.88235295]]\n",
      "\n",
      "  [[ 0.98039216  0.98039216  0.97254902]\n",
      "   [ 1.          1.          0.99215686]\n",
      "   [ 0.98039216  0.98039216  0.97254902]\n",
      "   ..., \n",
      "   [ 0.92156863  0.90980393  0.87450981]\n",
      "   [ 0.93333334  0.92156863  0.88627452]\n",
      "   [ 0.89803922  0.88627452  0.8509804 ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.82745099  0.80392158  0.74901962]\n",
      "   [ 0.84313726  0.81960785  0.7647059 ]\n",
      "   [ 0.83137256  0.80784315  0.75294119]\n",
      "   ..., \n",
      "   [ 0.75294119  0.73725492  0.7019608 ]\n",
      "   [ 0.78823531  0.77254903  0.73725492]\n",
      "   [ 0.7764706   0.76078433  0.72549021]]\n",
      "\n",
      "  [[ 0.84313726  0.81960785  0.7647059 ]\n",
      "   [ 0.85882354  0.83529413  0.78039217]\n",
      "   [ 0.84705883  0.82352942  0.76862746]\n",
      "   ..., \n",
      "   [ 0.77254903  0.75686276  0.72156864]\n",
      "   [ 0.79607844  0.78039217  0.74509805]\n",
      "   [ 0.7764706   0.76078433  0.72549021]]\n",
      "\n",
      "  [[ 0.84705883  0.82352942  0.76862746]\n",
      "   [ 0.86274511  0.8392157   0.78431374]\n",
      "   [ 0.84313726  0.81960785  0.7647059 ]\n",
      "   ..., \n",
      "   [ 0.74901962  0.73333335  0.69803923]\n",
      "   [ 0.78431374  0.76862746  0.73333335]\n",
      "   [ 0.78039217  0.7647059   0.72941178]]]\n",
      "\n",
      "\n",
      " [[[ 0.99607843  0.98039216  0.96862745]\n",
      "   [ 1.          0.99607843  0.98431373]\n",
      "   [ 0.98823529  0.97647059  0.95686275]\n",
      "   ..., \n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]]\n",
      "\n",
      "  [[ 1.          0.99215686  0.97254902]\n",
      "   [ 1.          1.          0.98039216]\n",
      "   [ 0.99607843  0.98431373  0.96470588]\n",
      "   ..., \n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]]\n",
      "\n",
      "  [[ 0.98431373  0.97254902  0.94509804]\n",
      "   [ 1.          0.98823529  0.96078432]\n",
      "   [ 0.97647059  0.96470588  0.93725491]\n",
      "   ..., \n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.72156864  0.71764708  0.70980394]\n",
      "   [ 0.71764708  0.71372551  0.70588237]\n",
      "   [ 0.70588237  0.6901961   0.68627453]\n",
      "   ..., \n",
      "   [ 0.50196081  0.49803922  0.47843137]\n",
      "   [ 0.52156866  0.51764709  0.49803922]\n",
      "   [ 0.5529412   0.5411765   0.52156866]]\n",
      "\n",
      "  [[ 0.73333335  0.72941178  0.72156864]\n",
      "   [ 0.73333335  0.72941178  0.72156864]\n",
      "   [ 0.72156864  0.70588237  0.7019608 ]\n",
      "   ..., \n",
      "   [ 0.52156866  0.51764709  0.49803922]\n",
      "   [ 0.52156866  0.51764709  0.49803922]\n",
      "   [ 0.53725493  0.52549022  0.50588238]]\n",
      "\n",
      "  [[ 0.74901962  0.74509805  0.73725492]\n",
      "   [ 0.74509805  0.74117649  0.73333335]\n",
      "   [ 0.73333335  0.71764708  0.71372551]\n",
      "   ..., \n",
      "   [ 0.56078434  0.55686277  0.53725493]\n",
      "   [ 0.56078434  0.55686277  0.53725493]\n",
      "   [ 0.58039218  0.56862748  0.54901963]]]\n",
      "\n",
      "\n",
      " [[[ 0.70588237  0.75294119  0.56470591]\n",
      "   [ 0.70980394  0.75686276  0.57647061]\n",
      "   [ 0.68235296  0.72941178  0.54901963]\n",
      "   ..., \n",
      "   [ 0.25098041  0.25490198  0.18431373]\n",
      "   [ 0.26274511  0.26666668  0.20392157]\n",
      "   [ 0.30588236  0.30980393  0.24705882]]\n",
      "\n",
      "  [[ 0.67058825  0.71764708  0.53725493]\n",
      "   [ 0.73725492  0.78039217  0.61176473]\n",
      "   [ 0.74509805  0.78823531  0.61960787]\n",
      "   ..., \n",
      "   [ 0.19607843  0.2         0.12941177]\n",
      "   [ 0.20392157  0.20784314  0.13725491]\n",
      "   [ 0.24313726  0.24705882  0.18431373]]\n",
      "\n",
      "  [[ 0.73725492  0.78039217  0.6156863 ]\n",
      "   [ 0.7647059   0.80784315  0.64313728]\n",
      "   [ 0.75294119  0.79607844  0.63921571]\n",
      "   ..., \n",
      "   [ 0.19215687  0.2         0.11764706]\n",
      "   [ 0.20392157  0.20784314  0.13725491]\n",
      "   [ 0.25490198  0.25882354  0.1882353 ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.82352942  0.84705883  0.73725492]\n",
      "   [ 0.84313726  0.85882354  0.75294119]\n",
      "   [ 0.82352942  0.84313726  0.72549021]\n",
      "   ..., \n",
      "   [ 0.8392157   0.84705883  0.79215688]\n",
      "   [ 0.86274511  0.87058824  0.81568629]\n",
      "   [ 0.85882354  0.86666667  0.81176472]]\n",
      "\n",
      "  [[ 0.83137256  0.85882354  0.73725492]\n",
      "   [ 0.85490197  0.87450981  0.75686276]\n",
      "   [ 0.8392157   0.85882354  0.74117649]\n",
      "   ..., \n",
      "   [ 0.87058824  0.87843138  0.82352942]\n",
      "   [ 0.88235295  0.89019608  0.83529413]\n",
      "   [ 0.86274511  0.87058824  0.81568629]]\n",
      "\n",
      "  [[ 0.83137256  0.85882354  0.73725492]\n",
      "   [ 0.8509804   0.87058824  0.75294119]\n",
      "   [ 0.8392157   0.85882354  0.74117649]\n",
      "   ..., \n",
      "   [ 0.85882354  0.86666667  0.81176472]\n",
      "   [ 0.87450981  0.88235295  0.82745099]\n",
      "   [ 0.85882354  0.86666667  0.81176472]]]]\n",
      "[[ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "data_x = np.array(data_x, dtype='float32')\n",
    "data_y = np.array(data_y, dtype='float32')\n",
    "\n",
    "print(data_x[:10])\n",
    "print(data_y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "360\n",
      "360\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "order = [i for i in range(len(data_x))]\n",
    "random.shuffle(order)\n",
    "\n",
    "print(len(order))\n",
    "n=int(len(data_x)*9/10)\n",
    "print(n)\n",
    "\n",
    "train_x = [data_x[i] for i in order[0:n]]\n",
    "train_y = [data_y[i] for i in order[0:n]]\n",
    "\n",
    "test_x = [data_x[i] for i in order[n:]]\n",
    "test_y = [data_y[i] for i in order[n:]]\n",
    "\n",
    "print(len(train_x))\n",
    "print(len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 124, 124, 32)\n",
      "(?, 30, 30, 64)\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default(), tf.device('/gpu:0'):\n",
    "    labels = tf.placeholder('float', [None, 2])\n",
    "    inputs = tf.placeholder('float', [None, 500, 500, 3])   \n",
    "#     inputs = tf.placeholder('float', [None, 500*500*3])                        \n",
    "\n",
    "    def weight_variable(shape): #定义一个函数，用于初始化所有的权值 W\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(shape): #定义一个函数，用于初始化所有的偏置项 b\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def conv2d(x, W): #定义一个函数，用于构建卷积层\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    def max_pool(x): #定义一个函数，用于构建池化层\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    #构建网络\n",
    "#     x_image = tf.reshape(inputs, [-1,500,500,3])         #转换输入数据shape,以便于用于网络中\n",
    "    W_conv1 = weight_variable([5, 5, 3, 32])      \n",
    "    b_conv1 = bias_variable([32])       \n",
    "    h_conv1 = tf.nn.relu(conv2d(inputs, W_conv1) + b_conv1)     #第一个卷积层\n",
    "    h_pool1 = max_pool(h_conv1)                                  #第一个池化层\n",
    "    print(h_pool1.shape)#(?, 124, 124, 32)\n",
    "\n",
    "    W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)      #第二个卷积层\n",
    "    h_pool2 = max_pool(h_conv2)                                   #第二个池化层\n",
    "    print(h_pool2.shape)#(?, 30, 30, 64)\n",
    "\n",
    "    # 第三层 是个全连接层,输入维数125*125*64, 输出维数为1024\n",
    "    W_fc1 = weight_variable([30 * 30 * 64, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 30*30*64])              #reshape成向量\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)    #第一个全连接层\n",
    "\n",
    "    keep_prob = tf.placeholder(\"float\") \n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)                  #dropout层\n",
    "\n",
    "    W_fc2 = weight_variable([1024, 2])\n",
    "    b_fc2 = bias_variable([2])\n",
    "#     y_predict=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)   #softmax层\n",
    "    y_predict=tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "#     cross_entropy = -tf.reduce_sum(labels*tf.log(y_predict))     #交叉熵\n",
    "#     train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)    #梯度下降法\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y_predict, labels = labels))\n",
    "    optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)    \n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(y_predict,1), tf.argmax(labels,1))    \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))    #精确度计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 5.702176\n",
      "accuarcy is  0.48\n",
      "Loss at step 50: 0.776365\n",
      "accuarcy is  0.58\n",
      "Loss at step 100: 0.138931\n",
      "accuarcy is  0.98\n",
      "Loss at step 150: 0.070234\n",
      "accuarcy is  0.98\n",
      "Loss at step 200: 0.038352\n",
      "accuarcy is  1.0\n",
      "Loss at step 250: 0.023218\n",
      "accuarcy is  1.0\n",
      "Loss at step 300: 0.015201\n",
      "accuarcy is  1.0\n",
      "Loss at step 350: 0.010549\n",
      "accuarcy is  1.0\n",
      "Loss at step 400: 0.007656\n",
      "accuarcy is  1.0\n",
      "Loss at step 450: 0.005675\n",
      "accuarcy is  1.0\n",
      "Loss at step 500: 0.004378\n",
      "accuarcy is  1.0\n"
     ]
    }
   ],
   "source": [
    "batch_start = 0\n",
    "batch_size = 50\n",
    "training_steps = 501\n",
    "\n",
    "# print(test_y[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "#     saver.restore(session, tf.train.latest_checkpoint('lstm_check'))\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(training_steps): \n",
    "        if(batch_start+batch_size>=len(train_x)):\n",
    "            order = [i for i in range(len(train_x))]\n",
    "            random.shuffle(order)\n",
    "            train_x = [train_x[i] for i in order]\n",
    "            train_y = [train_y[i] for i in order]\n",
    "            batch_start = 0        \n",
    "        feed_x = train_x[batch_start:batch_start+batch_size]\n",
    "        feed_y = train_y[batch_start:batch_start+batch_size]\n",
    "#         print(feed_x.shape)\n",
    "#         print(feed_y.shape)\n",
    "        feed_dict = {inputs: feed_x, labels: feed_y, keep_prob:1.0}\n",
    "        _, l, predictions, accur = session.run([optimizer, loss, labels, accuracy], feed_dict = feed_dict)\n",
    "        if (step % 50 == 0):\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('accuarcy is ', accur)\n",
    "#             saver.save(session, 'lstm_check/my-model-1.ckpt', global_step=step+1000)\n",
    "    test_loss, test_logits, test_accur = session.run([loss,labels,accuracy], feed_dict={inputs: test_x, labels: test_y, keep_prob:1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss,  0.564804  test accur is  0.825\n"
     ]
    }
   ],
   "source": [
    "print('test loss, ', test_loss, ' test accur is ', test_accur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
