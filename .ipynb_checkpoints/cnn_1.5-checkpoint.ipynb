{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyi/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixiv/id_0 is completed.\n",
      "pixiv/id_1 is completed.\n",
      "pixiv/id_2 is completed.\n",
      "pixiv/id_3 is completed.\n",
      "pixiv/id_4 is completed.\n",
      "pixiv/id_5 is completed.\n",
      "pixiv/id_6 is completed.\n",
      "pixiv/id_7 is completed.\n",
      "pixiv/id_8 is completed.\n",
      "1800\n",
      "1800\n"
     ]
    }
   ],
   "source": [
    "data_x, data_y = [], []\n",
    "num_id = 9\n",
    "\n",
    "for i in range(0, num_id):\n",
    "    folder = 'pixiv/id_'+str(i)\n",
    "    for img_file in os.listdir(folder):\n",
    "        img=np.array(Image.open(folder+'/'+img_file))\n",
    "        img = np.reshape(img, len(img)*len(img[0])*len(img[0][0]))\n",
    "        data_x.append(img/255.0)\n",
    "        \n",
    "        label = [0 for i in range(0, num_id)]\n",
    "        label[i]=1\n",
    "        data_y.append(label)   \n",
    "    print(folder+' is completed.')\n",
    "\n",
    "print(len(data_x))\n",
    "print(len(data_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n",
      "4\n",
      "pixiv/id_0 is completed.\n",
      "pixiv/id_1 is completed.\n",
      "pixiv/id_2 is completed.\n",
      "pixiv/id_3 is completed.\n",
      "pixiv/id_4 is completed.\n",
      "pixiv/id_5 is completed.\n",
      "pixiv/id_6 is completed.\n",
      "pixiv/id_7 is completed.\n",
      "500\n",
      "500\n",
      "4\n",
      "500\n",
      "500\n",
      "4\n",
      "500\n",
      "500\n",
      "4\n",
      "500\n",
      "500\n",
      "4\n",
      "500\n",
      "500\n",
      "4\n",
      "500\n",
      "500\n",
      "4\n",
      "500\n",
      "500\n",
      "4\n",
      "pixiv/id_8 is completed.\n"
     ]
    }
   ],
   "source": [
    "j = 0\n",
    "for i in range(0, num_id):\n",
    "    folder = 'pixiv/id_'+str(i)\n",
    "    for img_file in os.listdir(folder):\n",
    "        img=np.array(Image.open(folder+'/'+img_file))\n",
    "        if(len(img)*len(img[0])*len(img[0][0])!=750000):\n",
    "            print(len(img))\n",
    "            print(len(img[0]))\n",
    "            print(len(img[0][0]))\n",
    "\n",
    "         \n",
    "    print(folder+' is completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171\n",
      "1612\n",
      "1615\n",
      "1632\n",
      "1635\n",
      "1640\n",
      "1679\n",
      "1699\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data_x)):\n",
    "    if(len(data_x[i])!=750000):\n",
    "        print(i)\n",
    "#     print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2c5a5278e36d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# data_y = np.array(data_y, dtype='float32')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "data_x = np.array(data_x, dtype='float32')\n",
    "# data_y = np.array(data_y, dtype='float32')\n",
    "\n",
    "print(data_x[:10])\n",
    "print(data_y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "360\n",
      "360\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "order = [i for i in range(len(data_x))]\n",
    "random.shuffle(order)\n",
    "\n",
    "print(len(order))\n",
    "n=int(len(data_x)*9/10)\n",
    "print(n)\n",
    "\n",
    "train_x = [data_x[i] for i in order[0:n]]\n",
    "train_y = [data_y[i] for i in order[0:n]]\n",
    "\n",
    "test_x = [data_x[i] for i in order[n:]]\n",
    "test_y = [data_y[i] for i in order[n:]]\n",
    "\n",
    "print(len(train_x))\n",
    "print(len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 124, 124, 32)\n",
      "(?, 30, 30, 64)\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default(), tf.device('/gpu:0'):\n",
    "    labels = tf.placeholder('float', [None, 2])            #\n",
    "    inputs = tf.placeholder('float', [None, 500*500*3])                        #输入的数据占位符\n",
    "#     print(x.shape)\n",
    "#     print(y_actual.shape)\n",
    "\n",
    "    def weight_variable(shape): #定义一个函数，用于初始化所有的权值 W\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(shape): #定义一个函数，用于初始化所有的偏置项 b\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def conv2d(x, W): #定义一个函数，用于构建卷积层\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    def max_pool(x): #定义一个函数，用于构建池化层\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    #构建网络\n",
    "    x_image = tf.reshape(inputs, [-1,500,500,3])         #转换输入数据shape,以便于用于网络中\n",
    "    W_conv1 = weight_variable([5, 5, 3, 32])      \n",
    "    b_conv1 = bias_variable([32])       \n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)     #第一个卷积层\n",
    "    h_pool1 = max_pool(h_conv1)                                  #第一个池化层\n",
    "    print(h_pool1.shape)#(?, 124, 124, 32)\n",
    "\n",
    "    W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)      #第二个卷积层\n",
    "    h_pool2 = max_pool(h_conv2)                                   #第二个池化层\n",
    "    print(h_pool2.shape)#(?, 30, 30, 64)\n",
    "\n",
    "    # 第三层 是个全连接层,输入维数125*125*64, 输出维数为1024\n",
    "    W_fc1 = weight_variable([30 * 30 * 64, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 30*30*64])              #reshape成向量\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)    #第一个全连接层\n",
    "\n",
    "    keep_prob = tf.placeholder(\"float\") \n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)                  #dropout层\n",
    "\n",
    "    W_fc2 = weight_variable([1024, 2])\n",
    "    b_fc2 = bias_variable([2])\n",
    "    y_predict=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)   #softmax层\n",
    "\n",
    "    cross_entropy = -tf.reduce_sum(labels*tf.log(y_predict))     #交叉熵\n",
    "    train_step = tf.train.GradientDescentOptimizer(1e-3).minimize(cross_entropy)    #梯度下降法\n",
    "    correct_prediction = tf.equal(tf.argmax(y_predict,1), tf.argmax(labels,1))    \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))    #精确度计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  0.]\n",
      "[0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(test_y[0])\n",
    "test_y[0]=[0.0,1.0]\n",
    "print(test_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 1.0]\n",
      "Initialized\n",
      "Loss at step 0: 913.100647\n",
      "accuarcy is  0.22\n",
      "Loss at step 1: nan\n",
      "accuarcy is  1.0\n",
      "Loss at step 2: nan\n",
      "accuarcy is  1.0\n",
      "Loss at step 3: nan\n",
      "accuarcy is  1.0\n",
      "Loss at step 4: nan\n",
      "accuarcy is  1.0\n",
      "Loss at step 5: nan\n",
      "accuarcy is  1.0\n",
      "Loss at step 6: nan\n",
      "accuarcy is  1.0\n",
      "Loss at step 7: nan\n",
      "accuarcy is  1.0\n",
      "Loss at step 8: nan\n",
      "accuarcy is  1.0\n",
      "Loss at step 9: nan\n",
      "accuarcy is  1.0\n",
      "Loss at step 10: nan\n",
      "accuarcy is  1.0\n",
      "Loss at step 11: nan\n",
      "accuarcy is  1.0\n",
      "Loss at step 12: nan\n",
      "accuarcy is  1.0\n",
      "Loss at step 13: nan\n",
      "accuarcy is  1.0\n",
      "Loss at step 14: nan\n",
      "accuarcy is  1.0\n",
      "Loss at step 15: nan\n",
      "accuarcy is  1.0\n",
      "Loss at step 16: nan\n",
      "accuarcy is  1.0\n",
      "Loss at step 17: nan\n",
      "accuarcy is  1.0\n",
      "Loss at step 18: nan\n",
      "accuarcy is  1.0\n",
      "Loss at step 19: nan\n",
      "accuarcy is  1.0\n"
     ]
    }
   ],
   "source": [
    "batch_start = 0\n",
    "batch_size = 50\n",
    "training_steps = 20\n",
    "\n",
    "print(test_y[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "#     saver.restore(session, tf.train.latest_checkpoint('lstm_check'))\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(training_steps): \n",
    "        if(batch_start+batch_size>=len(train_x)):\n",
    "            order = [i for i in range(len(train_x))]\n",
    "            random.shuffle(order)\n",
    "            train_x = [train_x[i] for i in order]\n",
    "            train_y = [train_y[i] for i in order]\n",
    "            batch_start = 0        \n",
    "        feed_x = train_x[batch_start:batch_start+batch_size]\n",
    "        feed_y = train_y[batch_start:batch_start+batch_size]\n",
    "#         print(feed_x.shape)\n",
    "#         print(feed_y.shape)\n",
    "        feed_dict = {inputs: feed_x, labels: feed_y, keep_prob:1.0}\n",
    "        _, l, predictions, accur = session.run([train_step, cross_entropy, labels, accuracy], feed_dict = feed_dict)\n",
    "        if (step % 1 == 0):\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('accuarcy is ', accur)\n",
    "#             saver.save(session, 'lstm_check/my-model-1.ckpt', global_step=step+1000)\n",
    "    test_loss, test_logits, test_accur = session.run([cross_entropy,labels,accuracy], feed_dict={inputs: test_x, labels: test_y, keep_prob:1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss,  nan  test accur is  0.975\n"
     ]
    }
   ],
   "source": [
    "print('test loss, ', test_loss, ' test accur is ', test_accur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
