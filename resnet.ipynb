{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyi/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "may be download data...\n",
      "Extracting your_mnist_dir/train-images-idx3-ubyte.gz\n",
      "Extracting your_mnist_dir/train-labels-idx1-ubyte.gz\n",
      "Extracting your_mnist_dir/t10k-images-idx3-ubyte.gz\n",
      "Extracting your_mnist_dir/t10k-labels-idx1-ubyte.gz\n",
      "read data finished\n",
      "start training...\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "===\n",
      "(120, 784)\n",
      "(120, 10)\n",
      "---\n",
      "(120, 784)\n",
      "...\n",
      "(256, 784)\n",
      "10,loss:2.253577, train accuracy:0.350000, test accuray:0.300781\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "===\n",
      "(120, 784)\n",
      "(120, 10)\n",
      "---\n",
      "(120, 784)\n",
      "...\n",
      "(256, 784)\n",
      "20,loss:0.846590, train accuracy:0.700000, test accuray:0.660156\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "===\n",
      "(120, 784)\n",
      "(120, 10)\n",
      "---\n",
      "(120, 784)\n",
      "...\n",
      "(256, 784)\n",
      "30,loss:0.684659, train accuracy:0.758333, test accuray:0.824219\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "===\n",
      "(120, 784)\n",
      "(120, 10)\n",
      "---\n",
      "(120, 784)\n",
      "...\n",
      "(256, 784)\n",
      "40,loss:0.358186, train accuracy:0.916667, test accuray:0.894531\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "===\n",
      "(120, 784)\n",
      "(120, 10)\n",
      "---\n",
      "(120, 784)\n",
      "...\n",
      "(256, 784)\n",
      "50,loss:0.311568, train accuracy:0.891667, test accuray:0.910156\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "===\n",
      "(120, 784)\n",
      "(120, 10)\n",
      "---\n",
      "(120, 784)\n",
      "...\n",
      "(256, 784)\n",
      "60,loss:0.314703, train accuracy:0.908333, test accuray:0.921875\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "===\n",
      "(120, 784)\n",
      "(120, 10)\n",
      "---\n",
      "(120, 784)\n",
      "...\n",
      "(256, 784)\n",
      "70,loss:0.246029, train accuracy:0.925000, test accuray:0.941406\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "===\n",
      "(120, 784)\n",
      "(120, 10)\n",
      "---\n",
      "(120, 784)\n",
      "...\n",
      "(256, 784)\n",
      "80,loss:0.223821, train accuracy:0.933333, test accuray:0.945312\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "===\n",
      "(120, 784)\n",
      "(120, 10)\n",
      "---\n",
      "(120, 784)\n",
      "...\n",
      "(256, 784)\n",
      "90,loss:0.241521, train accuracy:0.916667, test accuray:0.949219\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "(120, 784)\n",
      "train finished\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "print('may be download data...')\n",
    "mnist = input_data.read_data_sets(\"your_mnist_dir/\", one_hot=True)\n",
    "print('read data finished')\n",
    "\n",
    "is_training=True\n",
    "\n",
    "def tf_variable(shape, name=None):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1), name=name)\n",
    "\n",
    "\n",
    "def dense_connect(x, shape):\n",
    "    w = tf_variable(shape)\n",
    "    b = tf.Variable(tf.zeros([shape[1]]))\n",
    "    return tf.matmul(x, w) + b\n",
    "\n",
    "\n",
    "def batch_norm(inputs, is_training,is_conv_out=True,decay = 0.999):\n",
    "    scale = tf.Variable(tf.ones([inputs.get_shape()[-1]]))\n",
    "    beta = tf.Variable(tf.zeros([inputs.get_shape()[-1]]))\n",
    "    pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False)\n",
    "    pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False)\n",
    "    if is_training:\n",
    "        if is_conv_out:\n",
    "            batch_mean, batch_var = tf.nn.moments(inputs,[0,1,2])\n",
    "        else:\n",
    "            batch_mean, batch_var = tf.nn.moments(inputs,[0])   \n",
    "\n",
    "        train_mean = tf.assign(pop_mean,\n",
    "                               pop_mean * decay + batch_mean * (1 - decay))\n",
    "        train_var = tf.assign(pop_var,\n",
    "                              pop_var * decay + batch_var * (1 - decay))\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(inputs,\n",
    "                batch_mean, batch_var, beta, scale, 0.001)\n",
    "    else:\n",
    "        return tf.nn.batch_normalization(inputs,\n",
    "            pop_mean, pop_var, beta, scale, 0.001)\n",
    "\n",
    "\n",
    "def conv2d_with_batch_norm(x, filter_shape, stride):\n",
    "\n",
    "    filter_ = tf_variable(filter_shape)\n",
    "    conv = tf.nn.conv2d(x, filter=filter_, strides=[1, stride, stride, 1], padding=\"SAME\")\n",
    "    normed=batch_norm(conv, is_training)\n",
    "\n",
    "    return  tf.nn.relu(normed)\n",
    "\n",
    "\n",
    "def conv2d(x, filter_shape, stride):\n",
    "\n",
    "    out_channels = filter_shape[3]\n",
    "\n",
    "    conv = tf.nn.conv2d(x, filter=tf_variable(filter_shape), strides=[1, stride, stride, 1], padding=\"SAME\")\n",
    "    bias = tf.Variable(tf.zeros([out_channels]), name=\"bias\")\n",
    "\n",
    "    return tf.nn.relu(tf.nn.bias_add(conv,bias))\n",
    "\n",
    "\n",
    "def max_pool(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "def residual_block(x, out_channels, down_sample, projection=False):\n",
    "    in_channels = x.get_shape().as_list()[3]\n",
    "    if down_sample:\n",
    "        x = max_pool(x)\n",
    "\n",
    "    output = conv2d_with_batch_norm(x, [3, 3, in_channels, out_channels], 1)\n",
    "    output = conv2d_with_batch_norm(output, [3, 3, out_channels, out_channels], 1)\n",
    "\n",
    "    if in_channels != out_channels:\n",
    "        if projection:\n",
    "            # projection shortcut\n",
    "            input_ = conv2d(x, [1, 1, in_channels, out_channels], 2)\n",
    "        else:\n",
    "            # zero-padding\n",
    "            input_ = tf.pad(x, [[0,0], [0,0], [0,0], [0, out_channels - in_channels]])\n",
    "    else:\n",
    "        input_ = x\n",
    "\n",
    "    return output + input_\n",
    "\n",
    "\n",
    "def residual_group(name,x,num_block,out_channels):\n",
    "\n",
    "    assert num_block>=1,'num_block must greater than 1'\n",
    "\n",
    "    with tf.variable_scope('%s_head'%name):\n",
    "        output = residual_block(x, out_channels, True)\n",
    "\n",
    "    for i in range (num_block-1):\n",
    "        with tf.variable_scope('%s_%d' % (name,i+1)):\n",
    "            output = residual_block(output,out_channels, False)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def residual_net(inpt):\n",
    "\n",
    "    with tf.variable_scope('conv1'):\n",
    "        output = conv2d(inpt, [3, 3, 1, 16], 1)\n",
    "\n",
    "    output=residual_group('conv2', x=output,num_block=2,out_channels=16)\n",
    "\n",
    "    output=residual_group('conv3', x=output,num_block=2,out_channels=32)\n",
    "\n",
    "    #output=residual_group('conv4', x=output,num_block=2,out_channels=64)\n",
    "\n",
    "    with tf.variable_scope('fc'):\n",
    "        output=max_pool(output)\n",
    "\n",
    "        shape=output.get_shape().as_list()\n",
    "        i_shape=shape[1]*shape[2]*shape[3]\n",
    "\n",
    "        output=tf.reshape(output,[-1,i_shape])\n",
    "\n",
    "        return dense_connect(output, [i_shape, 10])\n",
    "\n",
    "\n",
    "def train_network(batch_size = 120,training_iters=100,learning_rate=0.001):\n",
    "\n",
    "    x = tf.placeholder(\"float\", [None, 784])#[batch_size,width,height,channels]\n",
    "    y = tf.placeholder(\"float\", [None, 10])#[batch_size,num_classes]\n",
    "    \n",
    "    x_image = tf.reshape(x, [-1,28,28,1])\n",
    "    pred = residual_net(x_image)\n",
    "\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "    accuracytr = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    accuracyte = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    tf.summary.scalar('cost', cost)\n",
    "    tf.summary.scalar('train_accuracy', accuracytr)    \n",
    "    tf.summary.scalar('test_accuracy', accuracyte)\n",
    "\n",
    "    merged = tf.summary.merge_all()  \n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    print('start training...')\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        swriter = tf.summary.FileWriter(\"your_summary_dir/\", sess.graph)  \n",
    "        step = 1\n",
    "        while step< training_iters:\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            #print(np.shape(batch_xs),np.shape(batch_ys))\n",
    "            batch_xs = np.array(batch_xs)\n",
    "#             batch_xs = np.reshape(batch_xs,[np.shape(batch_xs)[0],28,28,1])\n",
    "            print(batch_xs.shape)\n",
    "            \n",
    "            sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys})\n",
    "            if(step % 10 == 0):\n",
    "                print('===')\n",
    "                print(batch_xs.shape)\n",
    "                print(batch_ys.shape)\n",
    "                summary,acc = sess.run([merged,accuracytr], feed_dict={x: batch_xs, y: batch_ys})\n",
    "                swriter.add_summary(summary,step)  \n",
    "                \n",
    "                print('---')\n",
    "                print(batch_xs.shape)\n",
    "                summary,loss = sess.run([merged,cost], feed_dict={x: batch_xs, y: batch_ys})\n",
    "                swriter.add_summary(summary,step)  \n",
    "\n",
    "                batch_test=mnist.test.images[:256]\n",
    "                print('...')\n",
    "                print(batch_test.shape)\n",
    "                summary,ta=sess.run([merged,accuracyte], feed_dict={x:batch_test, y: mnist.test.labels[:256]})\n",
    "                swriter.add_summary(summary,step)  \n",
    "                print(\"%s,loss:%s, train accuracy:%s, test accuray:%s\"%(step,\"{:.6f}\".format(loss),\"{:.6f}\".format(acc),\"{:.6f}\".format(ta)))\n",
    "\n",
    "            step += 1\n",
    "    print(\"train finished\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    train_network()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
